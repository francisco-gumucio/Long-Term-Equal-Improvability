{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from sklearn import metrics\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "x_dim = 2\n",
    "hiddens = [x_dim + 1, 32, 64, 1]\n",
    "test_size = 0.2\n",
    "valid_size = 0.125\n",
    "batch_size = 10\n",
    "seq_len = 10\n",
    "l = 0.1\n",
    "c_hiddens = [x_dim + 1, 32, 64, 1]\n",
    "g_hidden_size = 64\n",
    "g_num_layers = 2\n",
    "d_hidden_size = 64\n",
    "d_num_layers = 2\n",
    "gan_epochs = 3000\n",
    "\n",
    "def to_tensor(z, x, y=None):\n",
    "    if torch.is_tensor(x):\n",
    "        zx = torch.cat([z, x], dim=1)\n",
    "    else:\n",
    "        zx = np.concatenate([z, x], axis=1)\n",
    "        zx = torch.FloatTensor(zx)\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = torch.FloatTensor(y)\n",
    "        return zx, y\n",
    "    return zx\n",
    "\n",
    "class TrueModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, seed=0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(hiddens[:-1], hiddens[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.optim = Adam(self.parameters())\n",
    "\n",
    "    def forward(self, zx):\n",
    "        return self.model(zx)\n",
    "\n",
    "    def predict(self, z, x):\n",
    "        zx = to_tensor(z, x)\n",
    "        pred = self(zx)\n",
    "        pred_y = pred.detach().round().cpu().numpy()\n",
    "        return pred_y\n",
    "\n",
    "    def fit(self, z, x, y, patience=10):\n",
    "        zx, y = to_tensor(z, x, y)\n",
    "\n",
    "        epoch, counter = 0, 0\n",
    "        best_loss = float('inf')\n",
    "        while True:\n",
    "            pred = self(zx)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            \n",
    "            epoch += 1\n",
    "            if loss.item() <= best_loss:\n",
    "                torch.save(self.state_dict(), self.path)\n",
    "                best_loss = loss.item()\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == patience:\n",
    "                    break\n",
    "        print(f\"TrueModel Fit Done in {epoch} epochs!\")\n",
    "\n",
    "    def sample(self, s, x, scale=0.8):\n",
    "        sx = to_tensor(s, x)\n",
    "        prob = self(sx)\n",
    "        y = torch.bernoulli(prob * scale)\n",
    "        return y.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Time: {duration:5.2f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(hiddens[:-1], hiddens[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.optim = Adam(self.parameters())\n",
    "\n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for param in self.parameters():\n",
    "            params.append(param.detach().cpu().flatten().numpy())\n",
    "        return np.hstack(params)\n",
    "\n",
    "    def forward(self, s_mb, x_mb):\n",
    "        sx_mb = torch.cat([s_mb, x_mb], dim=1)\n",
    "        return self.model(sx_mb)\n",
    "\n",
    "    def predict(self, s_mb, x_mb):\n",
    "        probs = self(s_mb, x_mb)\n",
    "        pred_y = probs.detach().round().cpu().numpy()\n",
    "        return pred_y\n",
    "\n",
    "    def sample(self, s, x, scale=1.0):\n",
    "        prob = self(s, x)\n",
    "        y = torch.bernoulli(prob * scale)\n",
    "        return y.detach().cpu().numpy()\n",
    "\n",
    "    @count_time\n",
    "    def fit(self, loader, valid_loader, save_path, device, patience=20):\n",
    "        epoch, counter = 0, 0\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        while True:\n",
    "            loss = 0.\n",
    "            for s_mb, x_mb, y_mb in loader:\n",
    "                s_mb = s_mb.to(device)\n",
    "                x_mb = x_mb.to(device)\n",
    "                y_mb = y_mb.to(device)\n",
    "\n",
    "                batch_loss = 0.\n",
    "                for i in range(x_mb.size(1)):\n",
    "                    pred_y_mb = self(s_mb, x_mb[:, i])\n",
    "                    batch_loss += self.loss_fn(pred_y_mb, y_mb[:, i])\n",
    "                loss += batch_loss.item()\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "            epoch += 1\n",
    "            valid_loss = self.eval(valid_loader, device)\n",
    "            if valid_loss <= best_loss:\n",
    "                torch.save(self.state_dict(), save_path)\n",
    "                best_loss = valid_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == patience:\n",
    "                    break\n",
    "            \n",
    "            if epoch == 1 or epoch % 100 == 0:\n",
    "                print(f'{epoch:6.0f} | loss: {loss:6.4f}')\n",
    "        print(f\"Classifier Fit Done in {epoch} epochs!\")\n",
    "\n",
    "    def eval(self, loader, device, verbose=False):\n",
    "        loss = 0.\n",
    "        for s_mb, x_mb, y_mb in loader:\n",
    "            s_mb = s_mb.to(device)\n",
    "            x_mb = x_mb.to(device)\n",
    "            y_mb = y_mb.to(device)\n",
    "\n",
    "            batch_loss = 0.\n",
    "            for i in range(x_mb.size(1)):\n",
    "                pred_y_mb = self(s_mb, x_mb[:, i])\n",
    "                batch_loss += self.loss_fn(pred_y_mb, y_mb[:, i])\n",
    "                loss += batch_loss.item()\n",
    "\n",
    "                if verbose:\n",
    "                    pred_y_mb = self.predict(s_mb, x_mb[:, i])\n",
    "                    true_y_mb = y_mb[:, i].cpu().numpy()\n",
    "                    s_mb_np = s_mb.cpu().numpy()\n",
    "\n",
    "                    acc = metrics.accuracy_score(true_y_mb, pred_y_mb) * 100\n",
    "                    fair = demographic_parity(s_mb_np, pred_y_mb)\n",
    "                    print(f\"Step: {i:6.0f}, ACC: {acc:6.2f}%, FAIR: {fair:6.2f}\\n\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_initial_data(n, seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    s0 = torch.bernoulli(torch.empty(n,1).uniform_(0,1)).numpy()\n",
    "    x0 = np.random.randn(n, 1) + np.sin(s0)\n",
    "    z0 = np.cos(x0) + np.random.randn(n, 1)\n",
    "    y = torch.bernoulli(torch.from_numpy(1 /(1+  np.exp(-x0 +z0))))\n",
    "    return torch.from_numpy(s0), to_tensor(x0, z0), y\n",
    "\n",
    "def sequential_data(s0, x0, y0, seq_len, hiddens, l, seed=0):\n",
    "    n = s0.size()[0]\n",
    "    model = TrueModel(hiddens, seed)\n",
    "    sx = to_tensor(s0, x0)\n",
    "    sx.requires_grad = True\n",
    "    sx = sx.to(dtype=torch.float32)\n",
    "    prob = model(sx)\n",
    "    loss = nn.BCELoss()(prob, torch.ones_like(prob))\n",
    "    loss.backward()\n",
    "    x = x0\n",
    "    y= y0\n",
    "    prevx = x.numpy()\n",
    "    prevy = y\n",
    "    s0 = s0.numpy()\n",
    "    nx = np.empty_like(s0)\n",
    "    nz = np.empty_like(s0)\n",
    "    ny = np.empty_like(s0)\n",
    "    for i in range(1, seq_len):\n",
    "        loss = nn.BCELoss()(prob, torch.ones_like(prob))\n",
    "        delta_y = prevy*loss\n",
    "        for j in range(n):\n",
    "            nx[j] = np.random.randn() + np.sin(s0[j]) + l*(prevx[j][0] - int(delta_y[j]))\n",
    "            nz[j] = np.cos(nx[j]) + np.random.randn() + l*(prevx[j][1] - int(delta_y[j]))\n",
    "        ny = torch.bernoulli(torch.from_numpy(1 /(1+  np.exp(-nx +nz))))\n",
    "        prevx = to_tensor(nx, nz)\n",
    "        x = torch.cat((x, prevx),0)\n",
    "        y = torch.cat((y, ny),0)\n",
    "        prevx = prevx.numpy()\n",
    "        prevy = ny\n",
    "    x = np.array(x, dtype=np.float32).reshape((n, seq_len, 2))\n",
    "    y = np.array(y, dtype=np.int32).reshape(n, seq_len, 1)\n",
    "    return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0, x0, y0 = gen_initial_data(5,0)\n",
    "x, y = sequential_data(s0, x0, y0, seq_len, hiddens, l, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s_train, s_test, x_train, x_test, y_train, y_test = train_test_split(s0, x, y, test_size=test_size, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.h0_linear = nn.Linear(in_size, hidden_size)\n",
    "        self.rnn = nn.GRU(in_size + 3, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, in_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x0, noise, s0, clf):\n",
    "        ss = torch.clone(s0)\n",
    "        ss = ss.to(x0.device)\n",
    "        s0 = torch.zeros(s0.size(0), 2).scatter_(1, s0.long(), torch.ones_like(s0))\n",
    "        s0 = s0.to(x0.device)\n",
    "\n",
    "        h0 = self.h0_linear(x0)\n",
    "        h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        yt = clf(ss, x0)\n",
    "        \n",
    "        xs, ys = [x0], [yt]\n",
    "        for i in range(noise.size(1)):\n",
    "            y_noise = torch.cat([s0, yt, noise[:, i]], dim=-1).unsqueeze(1)\n",
    "            output, h0 = self.rnn(y_noise, h0)\n",
    "            xt = self.sigmoid(self.linear(output).squeeze())\n",
    "            # xt = self.linear(output).squeeze()\n",
    "            yt = clf(ss, xt)\n",
    "\n",
    "            xs.append(xt)\n",
    "            ys.append(yt)\n",
    "\n",
    "        xs = torch.stack(xs, dim=1)\n",
    "        ys = torch.stack(ys, dim=1)\n",
    "        return xs, ys, ys.round().detach()\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.GRU(in_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hn = self.rnn(x)\n",
    "        output = self.linear(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DistributionDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(hiddens[:-1], hiddens[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        layers.pop()\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(c_hiddens)\n",
    "generator = Generator(x_dim, g_hidden_size, g_num_layers)\n",
    "discriminator = Discriminator(x_dim, d_hidden_size, d_num_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
